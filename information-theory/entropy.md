## Entropy (エントロピー)

エントロピーの説明は以下の記事がわかりやすい。cross entropyを理解するには、KL divergenceも勉強する必要あり。KL divergenceは二つの確率分布を掛け合わせる。忘れたから要復習。
* [Shannon Entropy, Information Gain, and Picking Balls from Buckets](https://medium.com/udacity/shannon-entropy-information-gain-and-picking-balls-from-buckets-5810d35d54b4)
